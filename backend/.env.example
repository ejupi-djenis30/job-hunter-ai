# ═══════════════════════════════════════
# LLM Provider Configuration
# ═══════════════════════════════════════

# Provider: "groq" or "deepseek"
LLM_PROVIDER=groq

# ─── API Keys (set the one for your chosen provider) ───
GROQ_API_KEY=your-groq-api-key
DEEPSEEK_API_KEY=your-deepseek-api-key

# ─── Model ───
# Groq:     moonshotai/kimi-k2-instruct-0905, llama-3.3-70b-versatile
# DeepSeek: deepseek-reasoner (R1, best reasoning, 128K ctx, 32K output)
#           deepseek-chat (V3.2, fast, 128K ctx)
LLM_MODEL=moonshotai/kimi-k2-instruct-0905

# ─── Generation Parameters ───
LLM_MAX_TOKENS=16384
LLM_TEMPERATURE=0.7
LLM_TOP_P=1.0
LLM_FREQUENCY_PENALTY=0.0
LLM_PRESENCE_PENALTY=0.0

# ─── DeepSeek Thinking Mode ───
# "true" = Chain-of-Thought reasoning (deepseek-reasoner only)
# Better accuracy, slower. Temperature/top_p controlled by model.
LLM_THINKING=false

# ─── Advanced: Override base URL (optional) ───
# LLM_BASE_URL=https://api.groq.com/openai/v1
