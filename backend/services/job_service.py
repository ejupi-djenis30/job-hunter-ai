from typing import Dict, Any
from sqlalchemy.orm import Session
from fastapi import HTTPException
from backend.repositories.job_repository import JobRepository
from backend.schemas import JobUpdate

class JobService:
    def __init__(self, db: Session):
        self.repo = JobRepository(db)

    def get_jobs_by_user(
        self, user_id: int, page: int, page_size: int,
        filters: Dict[str, Any]
    ) -> Dict[str, Any]:
        skip = (page - 1) * page_size
        
        items = self.repo.get_by_user_filtered(
            user_id, skip=skip, limit=page_size, **filters
        )
        
        # Remove pagination/sorting params before passing to count/stats
        stats_filters = filters.copy()
        stats_filters.pop("sort_by", None)
        stats_filters.pop("sort_order", None)

        total = self.repo.count_by_user_filtered(user_id, **stats_filters)
        stats = self.repo.get_stats_by_user_filtered(user_id, **stats_filters)
        
        return {
            "items": items,
            "total": total,
            "page": page,
            "pages": (total + page_size - 1) // page_size,
            "total_applied": stats["total_applied"],
            "avg_score": stats["avg_score"]
        }

    def create_job(self, user_id: int, job_in: dict):
        from backend.models import ScrapedJob
        
        # Split fields: scraped-job fields vs user-relationship fields
        scraped_fields = {
            "title": job_in.get("title", ""),
            "company": job_in.get("company", ""),
            "platform": job_in.get("platform", "manual"),
            "platform_job_id": job_in.get("platform_job_id", None) or str(hash(
                str(job_in.get("title", "")) + str(job_in.get("company", ""))
            )),
            "external_url": job_in.get("external_url", None),
            "description": job_in.get("description", None),
            "location": job_in.get("location", None),
            "workload": job_in.get("workload", None),
        }
        
        # Upsert or create ScrapedJob
        existing_scraped = (
            self.repo.db.query(ScrapedJob)
            .filter(
                ScrapedJob.platform == scraped_fields["platform"],
                ScrapedJob.platform_job_id == scraped_fields["platform_job_id"],
            )
            .first()
        )
        if not existing_scraped:
            scraped_job = ScrapedJob(**{k: v for k, v in scraped_fields.items() if v is not None})
            self.repo.db.add(scraped_job)
            self.repo.db.flush()
        else:
            scraped_job = existing_scraped
        
        # Create the user-specific Job record
        job_data = {
            "user_id": user_id,
            "scraped_job_id": scraped_job.id,
            "applied": job_in.get("applied", False),
            "is_scraped": job_in.get("is_scraped", False),
            "affinity_score": job_in.get("affinity_score", None),
            "search_profile_id": job_in.get("search_profile_id", None),
        }
        return self.repo.create(job_data)

    def update_job(self, user_id: int, job_id: int, updates: JobUpdate):
        job = self.repo.get(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        if job.user_id != user_id:
            raise HTTPException(status_code=403, detail="Not authorized")
        return self.repo.update(job, updates)

def get_job_service(db: Session) -> JobService:
    return JobService(db)
