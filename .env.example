# ═══════════════════════════════════════
# Job Hunter AI — Environment Variables
# ═══════════════════════════════════════
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# ─── Security ───
# Required: generate with: python -c "import secrets; print(secrets.token_urlsafe(64))"
SECRET_KEY=CHANGE_ME_GENERATE_WITH_openssl_rand_hex_32

# ─── LLM Provider ───
# Provider: groq, deepseek, ollama, or gemini
LLM_PROVIDER=gemini

# ─── API Keys (set the one for your chosen provider) ───
# For Groq:     set LLM_API_KEY to your Groq API key
# For DeepSeek: set LLM_API_KEY to your DeepSeek API key
# For Gemini:   set LLM_API_KEY to your Gemini API key
LLM_API_KEY=your-api-key-here

# ─── Model Override ───
# Groq:     moonshotai/kimi-k2-instruct-0905, llama-3.3-70b-versatile
# DeepSeek: deepseek-reasoner (R1), deepseek-chat (V3.2)
# Gemini:   gemini-2.0-flash, gemini-2.5-pro-preview-06-05
# LLM_MODEL=gemini-2.0-flash

# ─── Ollama Configuration ───
# OLLAMA_BASE_URL=http://localhost:11434/v1
# OLLAMA_MODEL=llama3

# ─── DeepSeek Thinking Mode ───
# "true" = Chain-of-Thought reasoning (deepseek-reasoner only)
# LLM_THINKING=false

# ─── Gemini Thinking Level ───
# OFF, LOW, MEDIUM, HIGH (only applies to Gemini thinking models)
# LLM_THINKING_LEVEL=MEDIUM

# ─── Advanced LLM Parameters ───
# LLM_MAX_TOKENS=16384
# LLM_TEMPERATURE=0.7
# LLM_TOP_P=1.0
# LLM_BASE_URL=https://api.groq.com/openai/v1

# ─── Database ───
# Default: SQLite for local dev. Set to PostgreSQL for production.
# DATABASE_URL=sqlite:///./job_hunter.db
# DATABASE_URL=postgresql://user:pass@localhost:5432/jobhunter

# ─── Server ───
# API_HOST=127.0.0.1
# API_PORT=8000

# ─── CORS ───
# Comma-separated origins (default: http://localhost:5173,http://localhost:8000)
CORS_ORIGINS=http://localhost:5173,http://localhost:8000

# ─── Logging ───
# LOG_LEVEL=INFO

# ─── Docker / Postgres (only for docker-compose) ───
# POSTGRES_USER=postgres
# POSTGRES_PASSWORD=CHANGE_ME_USE_A_STRONG_PASSWORD
# POSTGRES_DB=jobhunter
